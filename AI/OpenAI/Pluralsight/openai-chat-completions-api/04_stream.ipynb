{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import time   \n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 23.96\n",
      "Response:\n",
      "ChatCompletion(id='chatcmpl-8QMavzNJblPsdbnAfWLoasUmfU0Jd', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976, 7778742049, 12586269025', role='assistant', function_call=None, tool_calls=None))], created=1701294073, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=208, prompt_tokens=49, total_tokens=257))\n"
     ]
    }
   ],
   "source": [
    "# Measure the time to compare\n",
    "start_time = time.time()\n",
    "\n",
    "# Make a request that calculates the Fibonacci sequence\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Give me the first 50 numbers in the fibonacci sequence, with a comma between each number and no newlines. E.g., 1, 1, 2, 3, 5, ...'}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Calculate execution time\n",
    "response_time = time.time() - start_time\n",
    "\n",
    "# Print results and execution time\n",
    "print(f\"Execution time: {response_time:.2f}\")\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: \n",
      "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976, 7778742049, 12586269025\n"
     ]
    }
   ],
   "source": [
    "output = response.choices[0].message.content\n",
    "print(f\"Results: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-8QMbK56SSLV67kGWqDSdNLNHqRF6v', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0)], created=1701294098, model='gpt-3.5-turbo-0613', object='chat.completion.chunk', system_fingerprint=None)\n",
      "ChatCompletionChunk(id='chatcmpl-8QMbK56SSLV67kGWqDSdNLNHqRF6v', choices=[Choice(delta=ChoiceDelta(content='2', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0)], created=1701294098, model='gpt-3.5-turbo-0613', object='chat.completion.chunk', system_fingerprint=None)\n",
      "ChatCompletionChunk(id='chatcmpl-8QMbK56SSLV67kGWqDSdNLNHqRF6v', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0)], created=1701294098, model='gpt-3.5-turbo-0613', object='chat.completion.chunk', system_fingerprint=None)\n"
     ]
    }
   ],
   "source": [
    "# A short example\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True  # Set stream=True to stream completions\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n",
      "'1'\n",
      "','\n",
      "' '\n",
      "'1'\n",
      "','\n",
      "' '\n",
      "'2'\n",
      "','\n",
      "' '\n",
      "'3'\n",
      "','\n",
      "' '\n",
      "'5'\n",
      "','\n",
      "' '\n",
      "'8'\n",
      "','\n",
      "' '\n",
      "'13'\n",
      "','\n",
      "' '\n",
      "'21'\n",
      "','\n",
      "' '\n",
      "'34'\n",
      "','\n",
      "' '\n",
      "'55'\n",
      "','\n",
      "' '\n",
      "'89'\n",
      "','\n",
      "' '\n",
      "'144'\n",
      "','\n",
      "' '\n",
      "'233'\n",
      "','\n",
      "' '\n",
      "'377'\n",
      "','\n",
      "' '\n",
      "'610'\n",
      "','\n",
      "' '\n",
      "'987'\n",
      "','\n",
      "' '\n",
      "'159'\n",
      "'7'\n",
      "','\n",
      "' '\n",
      "'258'\n",
      "'4'\n",
      "','\n",
      "' '\n",
      "'418'\n",
      "'1'\n",
      "','\n",
      "' '\n",
      "'676'\n",
      "'5'\n",
      "','\n",
      "' '\n",
      "'109'\n",
      "'46'\n",
      "','\n",
      "' '\n",
      "'177'\n",
      "'11'\n",
      "','\n",
      "' '\n",
      "'286'\n",
      "'57'\n",
      "','\n",
      "' '\n",
      "'463'\n",
      "'68'\n",
      "','\n",
      "' '\n",
      "'750'\n",
      "'25'\n",
      "','\n",
      "' '\n",
      "'121'\n",
      "'393'\n",
      "','\n",
      "' '\n",
      "'196'\n",
      "'418'\n",
      "','\n",
      "' '\n",
      "'317'\n",
      "'811'\n",
      "','\n",
      "' '\n",
      "'514'\n",
      "'229'\n",
      "','\n",
      "' '\n",
      "'832'\n",
      "'040'\n",
      "','\n",
      "' '\n",
      "'134'\n",
      "'626'\n",
      "'9'\n",
      "','\n",
      "' '\n",
      "'217'\n",
      "'830'\n",
      "'9'\n",
      "','\n",
      "' '\n",
      "'352'\n",
      "'457'\n",
      "'8'\n",
      "','\n",
      "' '\n",
      "'570'\n",
      "'288'\n",
      "'7'\n",
      "','\n",
      "' '\n",
      "'922'\n",
      "'746'\n",
      "'5'\n",
      "','\n",
      "' '\n",
      "'149'\n",
      "'303'\n",
      "'52'\n",
      "','\n",
      "' '\n",
      "'241'\n",
      "'578'\n",
      "'17'\n",
      "','\n",
      "' '\n",
      "'390'\n",
      "'881'\n",
      "'69'\n",
      "','\n",
      "' '\n",
      "'632'\n",
      "'459'\n",
      "'86'\n",
      "','\n",
      "' '\n",
      "'102'\n",
      "'334'\n",
      "'155'\n",
      "','\n",
      "' '\n",
      "'165'\n",
      "'580'\n",
      "'141'\n",
      "','\n",
      "' '\n",
      "'267'\n",
      "'914'\n",
      "'296'\n",
      "','\n",
      "' '\n",
      "'433'\n",
      "'494'\n",
      "'437'\n",
      "','\n",
      "' '\n",
      "'701'\n",
      "'408'\n",
      "'733'\n",
      "','\n",
      "' '\n",
      "'113'\n",
      "'490'\n",
      "'317'\n",
      "'0'\n",
      "','\n",
      "' '\n",
      "'183'\n",
      "'631'\n",
      "'190'\n",
      "'3'\n",
      "','\n",
      "' '\n",
      "'297'\n",
      "'121'\n",
      "'507'\n",
      "'3'\n",
      "','\n",
      "' '\n",
      "'480'\n",
      "'752'\n",
      "'697'\n",
      "'6'\n",
      "','\n",
      "' '\n",
      "'777'\n",
      "'874'\n",
      "'204'\n",
      "'9'\n",
      "','\n",
      "' '\n",
      "'125'\n",
      "'862'\n",
      "'690'\n",
      "'25'\n",
      "None\n",
      "Execution time: 25.60\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_reply_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # Print the delay and text\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Print the time delay and text received\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_reply_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_reply_content' is not defined"
     ]
    }
   ],
   "source": [
    "### Measure the time to compare\n",
    "start_time = time.time()\n",
    "\n",
    "# Make a request that calculates the Fibonacci sequence, but with stream set to True\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Give me the first 50 numbers in the Fibonacci sequence, with a comma between each number and no newlines. E.g., 1, 1, 2, 3, 5, ...'}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True   \n",
    ")\n",
    "\n",
    "# Create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "\n",
    "# Iterate through the stream of events\n",
    "for chunk in response:\n",
    "    chunk_time = time.time() - start_time  # Calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # Save event response    \n",
    "    chunk_message = chunk.choices[0].delta \n",
    "    pprint(chunk_message.content)\n",
    "    chunk_message = chunk.choices[0].delta  # Extract message\n",
    "    collected_messages.append(chunk_message.content)  # Save message\n",
    "    # print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # Print the delay and text\n",
    "\n",
    "# Print the time delay and text received\n",
    "print(f\"Execution time: {chunk_time:.2f}\")\n",
    "print(f\"Response: {full_reply_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
